#!/usr/bin/env python3
"""
Merge spread orders and trades into the reference backtest format
"""

import pandas as pd
import numpy as np
from pathlib import Path
import os

# File paths
data_dir = '/mnt/c/Users/krajcovic/Documents/Testing Data/ATS_data/test/parquet_files'
orders_file = 'DEM1-DEM2_orders_robust.parquet'
trades_file = 'DEM1-DEM2_trades_robust.parquet'

orders_path = os.path.join(data_dir, orders_file)
trades_path = os.path.join(data_dir, trades_file)

# Output path (matching reference format naming)
output_file = 'dem01-dem02_spread_tr_ba_data.parquet'
output_path = os.path.join(data_dir, output_file)

print("ğŸ”— Merging Spread Orders & Trades into Reference Format")
print("=" * 60)
print(f"ğŸ“ Input directory: {data_dir}")
print(f"ğŸ“Š Orders file: {orders_file}")
print(f"ğŸ’¹ Trades file: {trades_file}")
print(f"ğŸ“¦ Output file: {output_file}")

try:
    # Load the robust data files
    print(f"\nğŸ“‚ Loading data...")
    orders_df = pd.read_parquet(orders_path, engine='pyarrow')
    trades_df = pd.read_parquet(trades_path, engine='pyarrow')
    
    print(f"âœ… Orders loaded: {orders_df.shape}")
    print(f"âœ… Trades loaded: {trades_df.shape}")
    print(f"ğŸ“… Orders period: {orders_df.index.min()} to {orders_df.index.max()}")
    print(f"ğŸ“… Trades period: {trades_df.index.min()} to {trades_df.index.max()}")
    
    # Process trades data first
    print(f"\nğŸ”„ Processing trades data...")
    
    # Convert buy/sell columns to the reference format
    trades_processed = pd.DataFrame(index=trades_df.index)
    
    # Extract trade price and direction
    trades_processed['price'] = trades_df['buy'].fillna(trades_df['sell'])
    trades_processed['action'] = np.where(trades_df['buy'].notna(), 1.0, 
                                         np.where(trades_df['sell'].notna(), -1.0, np.nan))
    
    # Add trade metadata (using placeholder values similar to reference)
    trades_processed['volume'] = 1  # Default volume (could be enhanced)
    trades_processed['broker_id'] = 1441.0  # EEX broker ID
    trades_processed['count'] = 1  # Trade count
    trades_processed['tradeid'] = [f"spread_trade_{i}" for i in range(len(trades_processed))]
    
    # Remove any rows without valid trades
    trades_processed = trades_processed.dropna(subset=['price', 'action'])
    
    print(f"   âœ… Processed trades: {len(trades_processed)} valid executions")
    print(f"   ğŸ“Š Buy trades: {(trades_processed['action'] == 1.0).sum()}")
    print(f"   ğŸ“Š Sell trades: {(trades_processed['action'] == -1.0).sum()}")
    
    # Process orders data
    print(f"\nğŸ”„ Processing orders data...")
    
    # Rename columns to match reference format
    orders_processed = orders_df.rename(columns={
        'bid': 'b_price',
        'ask': 'a_price'
    })
    
    # Calculate mid price (column '0')
    orders_processed['0'] = (orders_processed['b_price'] + orders_processed['a_price']) / 2
    
    print(f"   âœ… Processed orders: {len(orders_processed)} market data points")
    print(f"   ğŸ“Š Mid price range: {orders_processed['0'].min():.3f} to {orders_processed['0'].max():.3f}")
    
    # Merge trades and orders on timestamp
    print(f"\nğŸ”— Merging trades and orders...")
    
    # Method 1: Use trades as base and forward-fill orders
    print(f"   ğŸ”„ Aligning order book data to trade timestamps...")
    
    # Get the union of all timestamps
    all_timestamps = orders_processed.index.union(trades_processed.index)
    
    # Reindex orders to all timestamps and forward-fill
    orders_aligned = orders_processed.reindex(all_timestamps).ffill()
    
    # Create the final merged DataFrame
    # Start with all timestamps that have either trades or orders
    merged_df = pd.DataFrame(index=all_timestamps)
    
    # Add trade data (will be NaN for non-trade timestamps)
    for col in ['price', 'volume', 'action', 'broker_id', 'count', 'tradeid']:
        merged_df[col] = trades_processed.reindex(all_timestamps)[col]
    
    # Add order book data (forward-filled to all timestamps)
    for col in ['b_price', 'a_price', '0']:
        merged_df[col] = orders_aligned[col]
    
    # Fill trade metadata for non-trade rows with NaN (matching reference behavior)
    trade_cols = ['price', 'volume', 'action', 'broker_id', 'count', 'tradeid']
    for col in trade_cols:
        if col in ['volume', 'count']:
            merged_df[col] = merged_df[col].astype('Int64')  # Nullable integer
        elif col == 'tradeid':
            merged_df[col] = merged_df[col].astype('object')
        else:
            merged_df[col] = merged_df[col].astype('float64')
    
    # Ensure column order matches reference format
    reference_columns = ['price', 'volume', 'action', 'broker_id', 'count', 'tradeid', 'b_price', 'a_price', '0']
    merged_df = merged_df[reference_columns]
    
    # Remove any rows where order book data is missing (shouldn't happen with forward-fill)
    original_len = len(merged_df)
    merged_df = merged_df.dropna(subset=['b_price', 'a_price', '0'])
    
    if len(merged_df) < original_len:
        print(f"   ğŸ§¹ Removed {original_len - len(merged_df)} rows with missing order data")
    
    print(f"   âœ… Final merged data: {merged_df.shape}")
    print(f"   ğŸ“Š Trade rows: {merged_df['price'].notna().sum()}")
    print(f"   ğŸ“Š Order-only rows: {merged_df['price'].isna().sum()}")
    
    # Verify data types match reference
    print(f"\nğŸ“Š Data type verification:")
    expected_types = {
        'price': 'float64',
        'volume': 'Int64', 
        'action': 'float64',
        'broker_id': 'float64',
        'count': 'Int64',
        'tradeid': 'object',
        'b_price': 'float64',
        'a_price': 'float64',
        '0': 'float64'
    }
    
    for col, expected_type in expected_types.items():
        actual_type = str(merged_df[col].dtype)
        print(f"   {col}: {actual_type} {'âœ…' if expected_type in actual_type or actual_type in expected_type else 'âš ï¸'}")
    
    # Save the merged file
    print(f"\nğŸ’¾ Saving merged file...")
    merged_df.to_parquet(output_path, engine='pyarrow', compression='snappy')
    
    # Verify the save
    test_df = pd.read_parquet(output_path, engine='pyarrow')
    print(f"âœ… Saved and verified: {test_df.shape}")
    
    # Save CSV backup
    csv_path = output_path.replace('.parquet', '.csv')
    merged_df.head(5000).to_csv(csv_path)  # Save first 5000 rows as sample
    print(f"ğŸ’¾ CSV sample saved: {Path(csv_path).name}")
    
    # Show sample of final result
    print(f"\nğŸ“‹ Sample of merged data:")
    print(merged_df.head(10))
    
    print(f"\nğŸ‰ SUCCESS! Merged file created in reference format")
    print(f"ğŸ“ Output: {output_path}")
    print(f"ğŸ“Š Format: {merged_df.shape} with {reference_columns}")
    print(f"ğŸ¯ Ready for backtest analysis!")
    
except Exception as e:
    print(f"âŒ Error: {e}")
    import traceback
    traceback.print_exc()